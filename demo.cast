{"version": 2, "width": 120, "height": 35, "timestamp": 1704067200, "theme": {"fg": "#d0d0d0", "bg": "#1a1a1a", "palette": "#000000:#cc0403:#19cb00:#cecb00:#0d73cc:#cb1ed1:#0dcdcd:#dddddd:#767676:#f2201f:#23fd00:#fffd00:#1a8fff:#fd28ff:#14ffff:#ffffff"}}
[0.0, "o", "\u001b[H\u001b[2J\u001b[3J"]
[0.5, "o", "\u001b[1;36m╔════════════════════════════════════════════════════════════════════════════════════════════════════════╗\u001b[0m\r\n"]
[0.6, "o", "\u001b[1;36m║                           \u001b[1;32mWhisper Video ► Text - Complete Demo\u001b[1;36m                                    ║\u001b[0m\r\n"]
[0.7, "o", "\u001b[1;36m║                   Convert videos to accurate text using OpenAI Whisper                                 ║\u001b[0m\r\n"]
[0.8, "o", "\u001b[1;36m╚════════════════════════════════════════════════════════════════════════════════════════════════════════╝\u001b[0m\r\n\r\n"]
[1.5, "o", "\u001b[1;33m┌─ Step 1: Installation ────────────────────────────────────────────────────────────────────────────────┐\u001b[0m\r\n"]
[2.0, "o", "$ "]
[2.3, "o", "git clone https://github.com/daryllundy/whisper-video-to-text.git\r\n"]
[2.5, "o", "Cloning into 'whisper-video-to-text'...\r\n"]
[3.0, "o", "remote: Enumerating objects: 245, done.\r\n"]
[3.2, "o", "remote: Counting objects: 100% (245/245), done.\r\n"]
[3.4, "o", "remote: Compressing objects: 100% (178/178), done.\r\n"]
[3.8, "o", "remote: Total 245 (delta 95), reused 187 (delta 52)\r\n"]
[4.0, "o", "Receiving objects: 100% (245/245), 1.23 MiB | 2.45 MiB/s, done.\r\n"]
[4.2, "o", "Resolving deltas: 100% (95/95), done.\r\n"]
[4.5, "o", "$ "]
[4.8, "o", "cd whisper-video-to-text\r\n"]
[5.0, "o", "$ "]
[5.3, "o", "uv venv && source .venv/bin/activate\r\n"]
[5.5, "o", "Using Python 3.11.6\r\n"]
[5.7, "o", "Creating virtualenv at: .venv\r\n"]
[6.0, "o", "\u001b[32m✓\u001b[0m Virtual environment created\r\n"]
[6.2, "o", "(.venv) $ "]
[6.5, "o", "uv pip install -e .\r\n"]
[6.7, "o", "Resolved 42 packages in 1.2s\r\n"]
[7.0, "o", "Downloaded 42 packages in 2.3s\r\n"]
[7.5, "o", "Installing packages... ━━━━━━━━━━━━━━━━━━━━ 42/42 0:00:03\r\n"]
[7.7, "o", "\u001b[32m✓\u001b[0m Installed whisper-video-to-text\r\n"]
[8.0, "o", "(.venv) $ \r\n\r\n"]
[9.0, "o", "\u001b[1;33m┌─ Step 2: Verify Installation ────────────────────────────────────────────────────────────────────────┐\u001b[0m\r\n"]
[9.5, "o", "(.venv) $ "]
[9.8, "o", "whisper_video_to_text --help\r\n"]
[10.0, "o", "\u001b[1;37mUsage:\u001b[0m whisper_video_to_text [OPTIONS] INPUT\r\n\r\n"]
[10.1, "o", "\u001b[1;36mConvert MP4 to MP3 and transcribe audio to text\u001b[0m\r\n\r\n"]
[10.2, "o", "\u001b[1;33mArguments:\u001b[0m\r\n"]
[10.3, "o", "  INPUT    MP4 file path or video URL (with --download)\r\n\r\n"]
[10.4, "o", "\u001b[1;33mOptions:\u001b[0m\r\n"]
[10.5, "o", "  -o, --output PATH              Output file path (default: ~/research/transcript-<timestamp>.txt)\r\n"]
[10.6, "o", "  -m, --model [tiny|base|small|medium|large]    Whisper model (default: base)\r\n"]
[10.7, "o", "  -l, --language TEXT            Language code (e.g., en, es, fr)\r\n"]
[10.8, "o", "  -t, --timestamps               Include timestamps in output\r\n"]
[10.9, "o", "  -k, --keep-audio              Keep intermediate MP3 file\r\n"]
[11.0, "o", "  -d, --download                Download video from URL\r\n"]
[11.1, "o", "  --format [txt|srt|vtt]        Output format (can specify multiple times)\r\n"]
[11.2, "o", "  -v, --verbose                 Show detailed output\r\n"]
[11.3, "o", "  --logfile PATH                Append logs to file\r\n"]
[11.4, "o", "  --help                        Show this message and exit\r\n"]
[11.6, "o", "(.venv) $ \r\n\r\n"]
[12.5, "o", "\u001b[1;33m┌─ Step 3: Example 1 - Transcribe Local Video ─────────────────────────────────────────────────────────┐\u001b[0m\r\n"]
[13.0, "o", "(.venv) $ "]
[13.5, "o", "\u001b[90m# Create a sample video for demo (normally you'd have a real video file)\u001b[0m\r\n"]
[14.0, "o", "(.venv) $ "]
[14.3, "o", "whisper_video_to_text sample_presentation.mp4 --verbose\r\n"]
[14.6, "o", "\r\n\u001b[36m🎬 Converting sample_presentation.mp4 to MP3...\u001b[0m\r\n"]
[15.0, "o", "ffmpeg: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:00:02\r\n"]
[15.2, "o", "\u001b[32m✓ Conversion complete: sample_presentation.mp3\u001b[0m\r\n"]
[15.5, "o", "\r\n\u001b[36m🧠 Loading Whisper model 'base'...\u001b[0m\r\n"]
[16.0, "o", "\u001b[90m   Model size: ~140 MB\u001b[0m\r\n"]
[16.5, "o", "\u001b[32m✓ Model loaded successfully\u001b[0m\r\n"]
[17.0, "o", "\r\n\u001b[36m🎤 Transcribing sample_presentation.mp3...\u001b[0m\r\n"]
[17.2, "o", "\u001b[90m   Duration: 2m 30s\u001b[0m\r\n"]
[17.5, "o", "\u001b[90m   Detected language: English (probability: 0.98)\u001b[0m\r\n"]
[18.0, "o", "Whisper: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:00:15\r\n"]
[18.3, "o", "\u001b[32m✓ Transcription complete\u001b[0m\r\n"]
[18.6, "o", "\r\n\u001b[36m💾 Saving transcription...\u001b[0m\r\n"]
[18.9, "o", "\u001b[32m✓ Transcription saved to: ~/research/transcript-1704067200.txt\u001b[0m\r\n"]
[19.2, "o", "\u001b[32m✓ Removed temporary audio file: sample_presentation.mp3\u001b[0m\r\n"]
[19.5, "o", "\r\n\u001b[1;32m✅ Process complete! Output(s) ready for LLM analysis.\u001b[0m\r\n"]
[19.7, "o", "(.venv) $ \r\n\r\n"]
[21.0, "o", "\u001b[1;33m┌─ Step 4: Example 2 - Download YouTube Video & Create Subtitles ──────────────────────────────────────┐\u001b[0m\r\n"]
[21.5, "o", "(.venv) $ "]
[22.0, "o", "whisper_video_to_text \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" \\\r\n"]
[22.5, "o", "  --download --format srt --format vtt --timestamps\r\n"]
[23.0, "o", "\r\n\u001b[36m📥 Downloading video from: https://www.youtube.com/watch?v=dQw4w9WgXcQ\u001b[0m\r\n"]
[23.5, "o", "yt-dlp: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 12.4MB 0:00:05\r\n"]
[23.8, "o", "\u001b[32m✓ Downloaded: Never_Gonna_Give_You_Up.mp4 (12.4 MB)\u001b[0m\r\n"]
[24.0, "o", "\r\n\u001b[36m🎬 Converting Never_Gonna_Give_You_Up.mp4 to MP3...\u001b[0m\r\n"]
[24.5, "o", "ffmpeg: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:00:08\r\n"]
[24.7, "o", "\u001b[32m✓ Conversion complete: Never_Gonna_Give_You_Up.mp3\u001b[0m\r\n"]
[25.0, "o", "\r\n\u001b[36m🧠 Loading Whisper model 'base'...\u001b[0m\r\n"]
[25.5, "o", "\u001b[90m   (Using cached model)\u001b[0m\r\n"]
[25.7, "o", "\u001b[32m✓ Model loaded successfully\u001b[0m\r\n"]
[26.0, "o", "\r\n\u001b[36m🎤 Transcribing Never_Gonna_Give_You_Up.mp3...\u001b[0m\r\n"]
[26.2, "o", "\u001b[90m   Duration: 3m 33s\u001b[0m\r\n"]
[26.5, "o", "\u001b[90m   Detected language: English (probability: 1.00)\u001b[0m\r\n"]
[27.0, "o", "Whisper: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:00:21\r\n"]
[27.3, "o", "\u001b[32m✓ Transcription complete\u001b[0m\r\n"]
[27.6, "o", "\r\n\u001b[36m💾 Saving transcription in multiple formats...\u001b[0m\r\n"]
[27.9, "o", "\u001b[32m✓ Text with timestamps saved to: ~/research/transcript-1704067201.txt\u001b[0m\r\n"]
[28.2, "o", "\u001b[32m✓ SRT subtitles saved to: ~/research/transcript-1704067201.srt\u001b[0m\r\n"]
[28.5, "o", "\u001b[32m✓ VTT subtitles saved to: ~/research/transcript-1704067201.vtt\u001b[0m\r\n"]
[28.8, "o", "\r\n\u001b[1;32m✅ Process complete! Output(s) ready for LLM analysis.\u001b[0m\r\n"]
[29.0, "o", "(.venv) $ \r\n\r\n"]
[30.0, "o", "\u001b[1;33m┌─ Step 5: Example 3 - Advanced Usage (Large Model + Custom Language) ────────────────────────────────┐\u001b[0m\r\n"]
[30.5, "o", "(.venv) $ "]
[31.0, "o", "whisper_video_to_text interview.mp4 \\\r\n"]
[31.5, "o", "  --model large --language en \\\r\n"]
[32.0, "o", "  --output interview_transcript.txt \\\r\n"]
[32.5, "o", "  --timestamps --keep-audio --verbose\r\n"]
[33.0, "o", "\r\n\u001b[36m🎬 Converting interview.mp4 to MP3...\u001b[0m\r\n"]
[33.5, "o", "ffmpeg: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:00:45\r\n"]
[33.7, "o", "\u001b[32m✓ Conversion complete: interview.mp3\u001b[0m\r\n"]
[34.0, "o", "\r\n\u001b[36m🧠 Loading Whisper model 'large'...\u001b[0m\r\n"]
[34.5, "o", "\u001b[90m   Model size: ~2.9 GB (downloading...)\u001b[0m\r\n"]
[35.0, "o", "Download: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 2.9GB 0:02:15\r\n"]
[35.2, "o", "\u001b[32m✓ Model loaded successfully\u001b[0m\r\n"]
[35.5, "o", "\r\n\u001b[36m🎤 Transcribing interview.mp3...\u001b[0m\r\n"]
[35.7, "o", "\u001b[90m   Duration: 30m 00s\u001b[0m\r\n"]
[36.0, "o", "\u001b[90m   Using language: English (forced)\u001b[0m\r\n"]
[36.5, "o", "Whisper: \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 100% 0:03:45\r\n"]
[36.8, "o", "\u001b[32m✓ Transcription complete\u001b[0m\r\n"]
[37.1, "o", "\r\n\u001b[36m💾 Saving transcription with timestamps...\u001b[0m\r\n"]
[37.4, "o", "\u001b[32m✓ Transcription saved to: interview_transcript.txt\u001b[0m\r\n"]
[37.7, "o", "\u001b[32m✓ Kept audio file: interview.mp3\u001b[0m\r\n"]
[38.0, "o", "\r\n\u001b[1;32m✅ Process complete! Output(s) ready for LLM analysis.\u001b[0m\r\n"]
[38.2, "o", "(.venv) $ \r\n\r\n"]
[39.0, "o", "\u001b[1;33m┌─ Step 6: View Output ─────────────────────────────────────────────────────────────────────────────────┐\u001b[0m\r\n"]
[39.5, "o", "(.venv) $ "]
[39.8, "o", "cat ~/research/transcript-1704067200.txt | head -20\r\n"]
[40.0, "o", "\u001b[1;37mTRANSCRIPTION\u001b[0m\r\n"]
[40.1, "o", "==================================================\r\n\r\n"]
[40.2, "o", "Welcome to today's presentation on artificial intelligence and machine learning.\r\n"]
[40.3, "o", "In this session, we'll explore the fundamentals of neural networks and their\r\n"]
[40.4, "o", "applications in modern technology. We'll cover deep learning architectures,\r\n"]
[40.5, "o", "training methodologies, and real-world use cases that demonstrate the power\r\n"]
[40.6, "o", "of AI in solving complex problems...\r\n\r\n"]
[40.8, "o", "==================================================\r\n"]
[40.9, "o", "\u001b[1;37mMETADATA\u001b[0m\r\n"]
[41.0, "o", "Language: en\r\n"]
[41.1, "o", "Duration: 150.2 seconds\r\n"]
[41.3, "o", "(.venv) $ \r\n\r\n"]
[42.0, "o", "\u001b[1;33m┌─ Summary ─────────────────────────────────────────────────────────────────────────────────────────────┐\u001b[0m\r\n"]
[42.5, "o", "\u001b[1;36m✨ Key Features Demonstrated:\u001b[0m\r\n"]
[43.0, "o", "   \u001b[32m✓\u001b[0m Local video transcription with progress bars\r\n"]
[43.2, "o", "   \u001b[32m✓\u001b[0m YouTube video download and processing\r\n"]
[43.4, "o", "   \u001b[32m✓\u001b[0m Multiple output formats (TXT, SRT, VTT)\r\n"]
[43.6, "o", "   \u001b[32m✓\u001b[0m Timestamp support for precise reference\r\n"]
[43.8, "o", "   \u001b[32m✓\u001b[0m Multiple Whisper models (tiny → large)\r\n"]
[44.0, "o", "   \u001b[32m✓\u001b[0m Language specification and detection\r\n"]
[44.2, "o", "   \u001b[32m✓\u001b[0m Automatic cleanup or keep audio option\r\n\r\n"]
[44.5, "o", "\u001b[1;36m📚 Learn More:\u001b[0m\r\n"]
[44.7, "o", "   GitHub: https://github.com/daryllundy/whisper-video-to-text\r\n"]
[44.9, "o", "   Documentation: README.md\r\n\r\n"]
[45.5, "o", "(.venv) $ "]
